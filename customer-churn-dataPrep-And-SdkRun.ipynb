{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Script MLOps on AWS with SageMaker Pipelines\n",
    "\n",
    "1. Use an existing template and modify it for your example as described in [1]. Modify the Abalone example with Customer Churn use case\n",
    "\n",
    "## Demo Scenario 1: SageMaker Studio GUI\n",
    "   1. Copy dataset using the code below\n",
    "   1. Modify build-spec, preprocessing, training (pipeline.py) and deployment code (by copying from github)\n",
    "   1. To trigger execution, git push changes. \n",
    "   1. Inspect graph, step input/output and logs.\n",
    "   1. Inspect code build\n",
    " \n",
    "## Demo Scenario 2: Trigger Execution SageMaker Studio GUI\n",
    "   1. Invoke pipeline from SDK using the code below (using the created get_pipeline() utility)\n",
    "   \n",
    "\n",
    "## Ref\n",
    "[1] Example: https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-pipelines/tabular/customizing_build_train_deploy_project/sagemaker-pipelines-customized-project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Customer Churn Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/hba-prj-mlops/hba-pipelines-demo-p-yjtrzutdxee3/sagemaker-hba-pipelines-demo-p-yjtrzutdxee3-modelbuild/pipelines/customer_churn\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/synthetic/churn.txt ./\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "prefix = 'sagemaker/DEMO-xgboost-churn'\n",
    "region = boto3.Session().region_name\n",
    "default_bucket = sagemaker.session.Session().default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "RawData = boto3.Session().resource('s3')\\\n",
    ".Bucket(default_bucket).Object(os.path.join(prefix, 'data/RawData.csv'))\\\n",
    ".upload_file('./churn.txt')\n",
    "\n",
    "print(os.path.join(\"s3://\",default_bucket, prefix, 'data/RawData.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke Pipeline from SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "default_bucket = sagemaker_session.default_bucket()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the module name or the path to your pipeline.py file.\n",
    "from pipelines.customer_churn.pipeline import get_pipeline\n",
    "\n",
    "model_package_group_name = f\"hba-pipelines-demo-p-yjtrzutdxee3\"\n",
    "pipeline_name = f\"hba-pipelines-demo-p-yjtrzutdxee3\"\n",
    "\n",
    "\n",
    "# These variables were defined the IAM role.\n",
    "pipeline = get_pipeline(\n",
    "    region=region,\n",
    "    role=role,\n",
    "    default_bucket=default_bucket,\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    pipeline_name=pipeline_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()\n",
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.list_steps()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
